{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"right\">by <a href=\"http://cse.iitkgp.ac.in/~adas/\">Abir Das</a> with help of <br> Ram Rakesh and Ankit Singh<br> </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the following details here\n",
    "** Name: ** `Harsh Maheshwari`<br/>\n",
    "** Roll Number: ** `16EE30010`<br/>\n",
    "** Department: ** `Electrical`<br/>\n",
    "** Email: ** `harshmaheshwari135@gmail.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file ([`assignment_01.ipynb`]()) to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: You will implement a fully connected neural network from scratch in this problem\n",
    "We marked places where you are expected to add/change your own code with **`##### write your code below #####`** comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "597wDiAvGvuB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are not supposed to import any other python library to work on this assignments.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''You are not supposed to import any other python library to work on this assignments.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B54oZmm1DNWe",
    "outputId": "8c59bd48-230d-4fb9-eba1-82471de363df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n"
     ]
    }
   ],
   "source": [
    "'''data is loaded from data directory.\n",
    "please don't remove the folder '''\n",
    "\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "# x_train = x_train.astype(float)\n",
    "x_train = x_train/255.0\n",
    "# print(x_train[6])\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = len(x_train)\n",
    "print(\"Number of training examples: {:d}\".format(train_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvVFhXNB5xrD"
   },
   "outputs": [],
   "source": [
    "'''Dimensions to be used for creating your model'''\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "input_dim = 784  # input dimension\n",
    "hidden_1_dim = 512  # hidden layer 1 dimension\n",
    "hidden_2_dim = 256  # hidden layer 2 dimension\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "'''Other hyperparameters'''\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hImaaujc5zXg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "#creating one hot vector representation of output classification\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "# print(y.shape, gt_indices.shape)\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "y_train_t = np.transpose(y_train)\n",
    "print(y_train_t.shape)\n",
    "# Number of mini-batches (as integer) in one epoch\n",
    "num_minibatches = np.floor(train_length/batch_size).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7lHWEWVaVlK",
    "outputId": "4ecb1bfc-4568-44cb-e109-57677da50eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of mini-batches 937 and total training data used in training:59968.\n"
     ]
    }
   ],
   "source": [
    "print(\"No of mini-batches {:d} and total training data used in training:\\\n",
    "{}.\".format(num_minibatches, num_minibatches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9HRf0Wj52cK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 784)\n",
      "(256, 512)\n",
      "(10, 256)\n"
     ]
    }
   ],
   "source": [
    "'''Randomly Initialize Weights  from standard normal distribution (i.e., mean = 0 and s.d. = 1.0).\n",
    "Use the dimesnions specified in the cell 3 to initialize your weights matrices. \n",
    "Use the nomenclature W1,W2 etc. (provided below) for the different weight matrices.'''\n",
    "'''w1 = 512x784    w2 = 256x512     w3 = 10x256'''\n",
    "########################## write your code below ##############################################\n",
    "W1 = np.random.normal(0,1.0,size=(512,784))\n",
    "W1_max = np.amax(W1)\n",
    "W1 = W1/W1_max\n",
    "W2 = np.random.normal(0,1.0,size=(256,512))\n",
    "W2_max = np.amax(W2)\n",
    "W2 = W2/W2_max\n",
    "W3 = np.random.normal(0,1.0,size=(10,256))\n",
    "W3_max = np.amax(W3)\n",
    "W3 = W3/W3_max\n",
    "print(W1.shape)\n",
    "print(W2.shape)\n",
    "print(W3.shape)\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmZRrEVb6CJy"
   },
   "outputs": [],
   "source": [
    "# Write a function which computes the softmax where X is vector of scores computed during forward pass\n",
    "def softmax(x):\n",
    "    ##############################write your code here #################################\n",
    "    exp_x = np.exp(x)\n",
    "    sum = np.sum(exp_x, axis=0)\n",
    "#     print (\"THe sum is:\", sum)\n",
    "    return (exp_x/sum)\n",
    "    ####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Gjz4yhwE6JQw",
    "outputId": "341578db-29a4-48ca-b0f8-a0343aadd24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, iteration: 0, Loss: 15.2516 \n",
      " Epoch: 1, iteration: 937, Loss: 14.7300 \n",
      " Epoch: 2, iteration: 1874, Loss: 14.7365 \n",
      " Epoch: 3, iteration: 2811, Loss: 14.7365 \n",
      " Epoch: 4, iteration: 3748, Loss: 14.7365 \n",
      " Epoch: 5, iteration: 4685, Loss: 14.7364 \n",
      " Epoch: 6, iteration: 5622, Loss: 14.7364 \n",
      " Epoch: 7, iteration: 6559, Loss: 14.7364 \n",
      " Epoch: 8, iteration: 7496, Loss: 14.7364 \n",
      " Epoch: 9, iteration: 8433, Loss: 14.7364 \n",
      " Epoch: 10, iteration: 9370, Loss: 14.7365 \n",
      " Epoch: 11, iteration: 10307, Loss: 14.7365 \n",
      " Epoch: 12, iteration: 11244, Loss: 14.7365 \n",
      " Epoch: 13, iteration: 12181, Loss: 14.7365 \n",
      " Epoch: 14, iteration: 13118, Loss: 14.7365 \n",
      " Epoch: 15, iteration: 14055, Loss: 14.7365 \n",
      " Epoch: 16, iteration: 14992, Loss: 14.7365 \n",
      " Epoch: 17, iteration: 15929, Loss: 14.7365 \n",
      " Epoch: 18, iteration: 16866, Loss: 14.7365 \n",
      " Epoch: 19, iteration: 17803, Loss: 14.7365 \n",
      " Epoch: 20, iteration: 18740, Loss: 14.7365 \n",
      " Epoch: 21, iteration: 19677, Loss: 14.7365 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "# print(x_train)\n",
    "no_of_iterations = 20000\n",
    "loss_list=[]\n",
    "i_epoch = 0\n",
    "for i_iter in range(no_of_iterations):\n",
    "    \n",
    "    ''''''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "#     y_batchinput = y_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    ########################## write your code below ##############################################\n",
    "    ######################### Forward Pass Block #####################################\n",
    "    '''Write the code for forward block of the neural network with 2 hidden layers.\n",
    "    Please stick to the notation below which follows the notation provided in the lecture slides.\n",
    "    Note that you are allowed to write the right hand sides of these variables in more than\n",
    "    one line if that is convenient for you.'''\n",
    "    \n",
    "    # first hidden layer implementation\n",
    "    a1 = np.dot(W1,np.transpose(x_batchinput))\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(a1,0)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.dot(W2, h1)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(a2,0)\n",
    "    #implement linear output layer\n",
    "    a3 = np.dot(W3, h2)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "#     print(softmax_score.shape)\n",
    "#     print(softmax_score)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ###############################################################################################\n",
    "\n",
    "    neg_log_softmax_score = -np.log(softmax_score+0.00000001) # The small number is added to avoid 0 input to log function\n",
    "    \n",
    "    # Compute and print loss\n",
    "    if i_iter%num_minibatches == 0:\n",
    "        loss = np.mean(np.diag(np.take(neg_log_softmax_score, gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size],\\\n",
    "                                       axis=1)))\n",
    "        print(\" Epoch: {:d}, iteration: {:d}, Loss: {:6.4f} \".format(i_epoch, i_iter, loss))\n",
    "        loss_list.append(loss)\n",
    "        i_epoch += 1\n",
    "        # Each 10th epoch reduce learning rate by a factor of 10\n",
    "        if i_epoch%10 == 0:\n",
    "            learning_rate /= 10.0\n",
    "     \n",
    "    ################################### Backpropagation Code Block #####################################\n",
    "    ''' Use the convention grad_{} for computing the gradients.\n",
    "    for e.g \n",
    "        grad_W1 for gradients w.r.t. weight W1\n",
    "        grad_w2 for gradients w.r.t. weights W2'''\n",
    "    ########################## write your code below ##############################################\n",
    "    # Gradient of cross-entropy loss w.r.t. preactivation of the output layer, dimension = 10xbatch_size\n",
    "    grad_softmax_score = softmax_score - y_train_t[:, batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    \n",
    "    # gradient w.r.t W3, size = 10x256, h2 = 256xbatch_size\n",
    "    grad_W3 = np.dot(grad_softmax_score, np.transpose(h2))#/batch_size\n",
    "    # gradient w.r.t h2 , size = 256xbatch_size,\n",
    "    grad_h2 = np.dot(np.transpose(W3), grad_softmax_score)\n",
    "    # gradient w.r.t a2,  size = 256xbatch_size\n",
    "    grad_a2 = grad_h2*relu_derivative(a2)\n",
    "    # gradient w.r.t W2, size = 256x512 , h1= 512xbatch_size\n",
    "    grad_W2 = np.dot(grad_a2, np.transpose(h1))#/batch_size\n",
    "    # gradient w.r.t h1, size = 512xbatch_size\n",
    "    grad_h1 = np.dot(np.transpose(W2), grad_a2)\n",
    "    # gradient w.r.t a1, size = 512xbatch_size\n",
    "    grad_a1 = grad_h1*relu_derivative(a1)\n",
    "    # gradient w.r.t W1, size = 512x784, x_batchinput = batch_sizex784\n",
    "    grad_W1 = np.dot(grad_a1, x_batchinput)#/batch_size\n",
    "    ###############################################################################################\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ Update Weights Block using SGD ####################################\n",
    "    W3 -= learning_rate * grad_W3\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    ####################################################################################################\n",
    "    \n",
    "#plotting the loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''Loading the test data from data/X_test.npy and data/y_test.npy.'''\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "# print(x_test)\n",
    "x_test = x_test / 255.0\n",
    "y_test = np.load('./data/y_test.npy')\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 88.99 %\n"
     ]
    }
   ],
   "source": [
    "batch_size_test = 100 # Deliberately taken 100 so that it divides the test data size\n",
    "num_minibatches = len(y_test)/batch_size_test\n",
    "test_correct = 0\n",
    "\n",
    "'''Only forward block code and compute softmax_score .'''\n",
    "for i_iter in range(int(num_minibatches)):\n",
    "    \n",
    "    '''Get one minibatch'''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    ######### copy only the forward pass block of your code and pass the x_batchinput to it and compute softmax_score ##########\n",
    "    # first hidden layer implementation\n",
    "    a1 = np.dot(W1,np.transpose(x_batchinput))\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(a1,0)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.dot(W2, h1)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(a2,0)\n",
    "    #implement linear output layer\n",
    "    a3 = np.dot(W3, h2)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "#     print(softmax_score.shape)\n",
    "    y_batchinput = y_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "#     print(y_batchinput)\n",
    "    y_pred = np.argmax(softmax_score, axis=0)\n",
    "#     print(y_pred)\n",
    "    num_correct_i_iter = np.sum(y_pred == y_batchinput)\n",
    "    test_correct += num_correct_i_iter\n",
    "print (\"Test accuracy is {:4.2f} %\".format(test_correct/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Hidden_MLP_New.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
