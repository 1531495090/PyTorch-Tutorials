{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.5000]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.6931, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "\n",
      " FC gradients :\n",
      "tensor([[-0.7000, -0.9750, -0.6000, -0.8250, -1.1250, -0.6750, -0.4000, -0.5250,\n",
      "         -0.3000],\n",
      "        [ 0.7000,  0.9750,  0.6000,  0.8250,  1.1250,  0.6750,  0.4000,  0.5250,\n",
      "          0.3000]], dtype=torch.float64)\n",
      "\n",
      "Conv Gradients :\n",
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]], dtype=torch.float64)\n",
      "\n",
      "Updated Conv Weights :\n",
      "Parameter containing:\n",
      "tensor([[[[0.1000, 0.2000, 0.3000],\n",
      "          [0.4000, 0.5000, 0.6000],\n",
      "          [0.7000, 0.8000, 0.9000]]]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "Updated FC Weights :\n",
      "Parameter containing:\n",
      "tensor([[0.1007, 0.2010, 0.3006, 0.4008, 0.5011, 0.6007, 0.7004, 0.8005, 0.9003],\n",
      "        [0.0993, 0.1990, 0.2994, 0.3992, 0.4989, 0.5993, 0.6996, 0.7995, 0.8997]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor([[0.5095, 0.4905]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.6837, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.5273, 0.4727]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.6662, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.5524, 0.4476]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.6421, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.5834, 0.4166]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.6132, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.6188, 0.3812]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.5814, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.6567, 0.3433]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.5487, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.6954, 0.3046]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.5167, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.7331, 0.2669]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.4870, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.7685, 0.2315]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.4602, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.8007, 0.1993]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.4370, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.8292, 0.1708]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.4172, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.8539, 0.1461]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.4006, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.8749, 0.1251]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3870, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.8926, 0.1074]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3757, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9074, 0.0926]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3666, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9197, 0.0803]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3591, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9300, 0.0700]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3529, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9385, 0.0615]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3479, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9456, 0.0544]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3437, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9516, 0.0484]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3402, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9566, 0.0434]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3374, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9608, 0.0392]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3349, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9644, 0.0356]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3329, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9675, 0.0325]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3312, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9701, 0.0299]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3297, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9724, 0.0276]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3284, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9743, 0.0257]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3273, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9760, 0.0240]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3264, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9775, 0.0225]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3256, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9788, 0.0212]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3249, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9799, 0.0201]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3242, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9809, 0.0191]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3237, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9818, 0.0182]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3232, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9826, 0.0174]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3227, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9833, 0.0167]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3223, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9840, 0.0160]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3220, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9846, 0.0154]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3217, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9851, 0.0149]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3214, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9856, 0.0144]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3211, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9860, 0.0140]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3209, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9864, 0.0136]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3207, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9867, 0.0133]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3205, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9871, 0.0129]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3203, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9874, 0.0126]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3201, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9877, 0.0123]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3200, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9879, 0.0121]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3198, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9882, 0.0118]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3197, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9884, 0.0116]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3195, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9886, 0.0114]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3194, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9888, 0.0112]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3193, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9890, 0.0110]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3192, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9892, 0.0108]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3191, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9894, 0.0106]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3190, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9895, 0.0105]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3189, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9897, 0.0103]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3189, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9898, 0.0102]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3188, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9900, 0.0100]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3187, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9901, 0.0099]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3186, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9902, 0.0098]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3186, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9903, 0.0097]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3185, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9904, 0.0096]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3184, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9906, 0.0094]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3184, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9907, 0.0093]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3183, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9908, 0.0092]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3183, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9909, 0.0091]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3182, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9910, 0.0090]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3182, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9910, 0.0090]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3181, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9911, 0.0089]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3181, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9912, 0.0088]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3180, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9913, 0.0087]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3180, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9914, 0.0086]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3179, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9915, 0.0085]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3179, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9915, 0.0085]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3178, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9916, 0.0084]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3178, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9917, 0.0083]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3178, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9918, 0.0082]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3177, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9918, 0.0082]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3177, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9919, 0.0081]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3176, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9920, 0.0080]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3176, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9920, 0.0080]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3176, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9921, 0.0079]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3175, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9922, 0.0078]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3175, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9922, 0.0078]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3175, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9923, 0.0077]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3174, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9924, 0.0076]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3174, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9924, 0.0076]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3174, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9925, 0.0075]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3173, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9925, 0.0075]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3173, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9926, 0.0074]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3173, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9926, 0.0074]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3172, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9927, 0.0073]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3172, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9927, 0.0073]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3172, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9928, 0.0072]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3172, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9928, 0.0072]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3171, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9929, 0.0071]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3171, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9929, 0.0071]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3171, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9930, 0.0070]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3170, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9930, 0.0070]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3170, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "tensor([[0.9931, 0.0069]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "\n",
      " Loss:\n",
      "tensor(0.3170, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "\n",
      " FC gradients :\n",
      "tensor([[-0.0106, -0.0148, -0.0091, -0.0126, -0.0171, -0.0103, -0.0062, -0.0081,\n",
      "         -0.0047],\n",
      "        [ 0.0106,  0.0148,  0.0091,  0.0126,  0.0171,  0.0103,  0.0062,  0.0081,\n",
      "          0.0047]], dtype=torch.float64)\n",
      "\n",
      "Conv Gradients :\n",
      "tensor([[[[-0.0025, -0.0037, -0.0028],\n",
      "          [-0.0040, -0.0059, -0.0044],\n",
      "          [-0.0032, -0.0047, -0.0035]]]], dtype=torch.float64)\n",
      "\n",
      "Updated Conv Weights :\n",
      "Parameter containing:\n",
      "tensor([[[[0.1059, 0.2086, 0.3064],\n",
      "          [0.4094, 0.5137, 0.6102],\n",
      "          [0.7075, 0.8109, 0.9081]]]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "Updated FC Weights :\n",
      "Parameter containing:\n",
      "tensor([[0.1900, 0.3253, 0.3772, 0.5061, 0.6447, 0.6869, 0.7515, 0.8676, 0.9387],\n",
      "        [0.0100, 0.0747, 0.2228, 0.2939, 0.3553, 0.5131, 0.6485, 0.7324, 0.8613]],\n",
      "       dtype=torch.float64, requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/.local/lib/python3.5/site-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(1,1,kernel_size=(3,3), stride = 1, padding = 1)\n",
    "        self.conv.weight = torch.nn.Parameter(data = torch.DoubleTensor([[[[0.1,0.2,0.3],[0.4,0.5,0.6],[0.7,0.8,0.9]]]]), requires_grad =True) #since the input channel and output channel is 1, dimension of filter is (1x1x3x3)\n",
    "        self.conv.bias = torch.nn.Parameter(data = torch.DoubleTensor([0]), requires_grad=True)\n",
    "        self.fc = nn.Linear(9,2)\n",
    "        self.fc.weight = torch.nn.Parameter(data = torch.DoubleTensor([[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]]), requires_grad = True)\n",
    "        self.fc.bias = torch.nn.Parameter(data=torch.DoubleTensor([0.0]), requires_grad=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "#         print(\"\\nThe output of conv is\")\n",
    "#         print(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.reshape(x,(-1,9))\n",
    "        x = self.fc(x)\n",
    "#         print(\"\\nThe output of FC is\")\n",
    "#         print(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "img = torch.from_numpy(np.ones((1,1,3,3))) #or torch.ones((1,1,3,3))\n",
    "label = torch.LongTensor([0])\n",
    "\n",
    "net = Net()\n",
    "# print(net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "for i in range(100):\n",
    "    output = net(img)\n",
    "    print(output)\n",
    "    \n",
    "    loss = criterion(output, label)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"\\n Loss:\")\n",
    "    print(loss)\n",
    "    if(i==0 or i == 99):\n",
    "\n",
    "        print(\"\\n FC gradients :\")\n",
    "        print(net.fc.weight.grad)\n",
    "\n",
    "        print(\"\\nConv Gradients :\")\n",
    "        print(net.conv.weight.grad)\n",
    "        print(\"\\nUpdated Conv Weights :\")\n",
    "        print(net.conv.weight)\n",
    "\n",
    "        print(\"\\nUpdated FC Weights :\")\n",
    "        print(net.fc.weight)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
